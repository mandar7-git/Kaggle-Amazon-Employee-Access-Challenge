{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* ### **Importing Libraries**\n  Add libraries as and when needed in the analysis.","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:13.577499Z","iopub.execute_input":"2021-08-28T09:15:13.57821Z","iopub.status.idle":"2021-08-28T09:15:14.556968Z","shell.execute_reply.started":"2021-08-28T09:15:13.578079Z","shell.execute_reply":"2021-08-28T09:15:14.556005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### **Reading the inputs**\n  We can read the given training and testing datasets using `read_csv` from Pandas.\n  ","metadata":{}},{"cell_type":"code","source":"train_file_path = '../input/amazon-employee-access-challenge/train.csv'\ntrain_df = pd.read_csv(train_file_path)\n\ntest_file_path = '../input/amazon-employee-access-challenge/test.csv'\ntest_df = pd.read_csv(test_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.558362Z","iopub.execute_input":"2021-08-28T09:15:14.55867Z","iopub.status.idle":"2021-08-28T09:15:14.756426Z","shell.execute_reply.started":"2021-08-28T09:15:14.558643Z","shell.execute_reply":"2021-08-28T09:15:14.755475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### **Understanding the data roughly**\n    `info()`, `head()`, `tail()`, `shape` and `describe()` can be used.  \n     Handle the null values (if any).\n     \n   ","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.758558Z","iopub.execute_input":"2021-08-28T09:15:14.758967Z","iopub.status.idle":"2021-08-28T09:15:14.786193Z","shell.execute_reply.started":"2021-08-28T09:15:14.758926Z","shell.execute_reply":"2021-08-28T09:15:14.78522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only datatype is `int64` (no categorical data), so we won't need to create dummy columns just to map the strings (This can be accomplished using `get_dummies` if needed). All features have numerical data, so `describe()` shows all of our columns.  ","metadata":{}},{"cell_type":"markdown","source":"**(Not needed for our dataset)**\n\nThe `ColumnTransformer` class from `sklearn.compose` is a pipeline usually used to automate two things:\n* imputing missing values in numerical data  \n* imputing missing values and applies proper encoding to categorical data","metadata":{}},{"cell_type":"code","source":"train_df.shape ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.787631Z","iopub.execute_input":"2021-08-28T09:15:14.787935Z","iopub.status.idle":"2021-08-28T09:15:14.795869Z","shell.execute_reply.started":"2021-08-28T09:15:14.787905Z","shell.execute_reply":"2021-08-28T09:15:14.794944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"32769 employees, 9 features","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.797198Z","iopub.execute_input":"2021-08-28T09:15:14.797518Z","iopub.status.idle":"2021-08-28T09:15:14.82133Z","shell.execute_reply.started":"2021-08-28T09:15:14.797485Z","shell.execute_reply":"2021-08-28T09:15:14.820682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.822509Z","iopub.execute_input":"2021-08-28T09:15:14.822792Z","iopub.status.idle":"2021-08-28T09:15:14.876653Z","shell.execute_reply.started":"2021-08-28T09:15:14.822759Z","shell.execute_reply":"2021-08-28T09:15:14.87557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the 'Non-Null Count' is equal to the number of rows in our training data for each of the column. Hence, we conclude that there are no null values. We could check this anyway using `.isnull().values.any()` and if it returns 'True', we can fill the fields appropriately using `fillna`. The `isnull().sum()` gives count of the number of null values for each column.","metadata":{}},{"cell_type":"code","source":"train_df.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.877902Z","iopub.execute_input":"2021-08-28T09:15:14.878184Z","iopub.status.idle":"2021-08-28T09:15:14.885872Z","shell.execute_reply.started":"2021-08-28T09:15:14.878156Z","shell.execute_reply":"2021-08-28T09:15:14.884793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### **Understanding the dataset using graphs**\n  Plot using **Matplotlib**, **Seaborn** and try to infer.  \n  Decide if any feature is **irrelevant** and is less likely to contribute to the outcome.   \n  Check if any of the features are **correlated**.   \n  ","metadata":{}},{"cell_type":"code","source":"# cols = ['ACTION','RESOURCE','MGR_ID','ROLE_ROLLUP_1','ROLE_ROLLUP_2','ROLE_DEPTNAME','ROLE_TITLE','ROLE_FAMILY_DESC','ROLE_FAMILY','ROLE_CODE']\ncols= train_df.columns\nfor i in cols:\n    train_df.hist(i)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:14.889822Z","iopub.execute_input":"2021-08-28T09:15:14.890249Z","iopub.status.idle":"2021-08-28T09:15:17.029731Z","shell.execute_reply.started":"2021-08-28T09:15:14.890217Z","shell.execute_reply":"2021-08-28T09:15:17.028568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all of the features have quite concentrated values.","metadata":{}},{"cell_type":"code","source":"for p in cols:\n n = len(pd.unique(train_df[p]))\n \n print(p,n)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:17.033352Z","iopub.execute_input":"2021-08-28T09:15:17.033653Z","iopub.status.idle":"2021-08-28T09:15:17.046057Z","shell.execute_reply.started":"2021-08-28T09:15:17.033625Z","shell.execute_reply":"2021-08-28T09:15:17.045036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`ROLE_FAMILY`, `ROLE_ROLLUP_1` & `ROLE_ROLLUP_2` are some features with relatively less unique values. So, we plot their scatterplots only.","metadata":{}},{"cell_type":"code","source":"a=['ROLE_FAMILY','ROLE_ROLLUP_1','ROLE_ROLLUP_2']\n\nfor j in a:\n      group = train_df.groupby(j) \n      plt.figure(figsize=(20,8))\n      plt.plot(group['ACTION'].agg(np.mean),'ro')\n      plt.xlabel(j)\n      plt.ylabel('Mean ACTION-->')\n      plt.show()\n    \n    \n# Used usual scatterplots before\nplt.figure(figsize=(20,8))\nsns.scatterplot(x=train_df[a[0]], y=train_df['ACTION'])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:17.047197Z","iopub.execute_input":"2021-08-28T09:15:17.047545Z","iopub.status.idle":"2021-08-28T09:15:17.897096Z","shell.execute_reply.started":"2021-08-28T09:15:17.047514Z","shell.execute_reply":"2021-08-28T09:15:17.896169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we check if any of the features are correlated (`corr()` can be used).","metadata":{}},{"cell_type":"code","source":"print(train_df.corr())\nsns.heatmap(train_df.corr())","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:17.898264Z","iopub.execute_input":"2021-08-28T09:15:17.898586Z","iopub.status.idle":"2021-08-28T09:15:18.310627Z","shell.execute_reply.started":"2021-08-28T09:15:17.898555Z","shell.execute_reply":"2021-08-28T09:15:18.30962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As long as the features are distinct (i!=j), we don't see any light areas indicating any major correlation in the datatset.  \nSome of the features that are slightly related:  \n`ROLE_TITLE` & `ROLE_CODE`   \n`ROLE_TITLE` & `ROLE_FAMILY_DESC` .","metadata":{}},{"cell_type":"markdown","source":"Let's check exact values of correlation with our label 'ACTION'.\n","metadata":{}},{"cell_type":"code","source":"train_df.corr()['ACTION'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.312265Z","iopub.execute_input":"2021-08-28T09:15:18.312559Z","iopub.status.idle":"2021-08-28T09:15:18.334668Z","shell.execute_reply.started":"2021-08-28T09:15:18.312533Z","shell.execute_reply":"2021-08-28T09:15:18.333821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['ROLE_ROLLUP_1'].value_counts().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.33558Z","iopub.execute_input":"2021-08-28T09:15:18.335863Z","iopub.status.idle":"2021-08-28T09:15:18.346966Z","shell.execute_reply.started":"2021-08-28T09:15:18.335836Z","shell.execute_reply":"2021-08-28T09:15:18.345646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some 'ROLE_ROLLUP_1' values do have higher count than others, hence don't drop this column.","metadata":{}},{"cell_type":"code","source":"train_df['ROLE_TITLE'].value_counts().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.348705Z","iopub.execute_input":"2021-08-28T09:15:18.349035Z","iopub.status.idle":"2021-08-28T09:15:18.361261Z","shell.execute_reply.started":"2021-08-28T09:15:18.349005Z","shell.execute_reply":"2021-08-28T09:15:18.360264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The feature **'ROLE_TITLE'** would probably lead to target leakage. The title is awarded after working for a few years in the company. But, the resources are provided when an employee joins (so this column can't possibly contribute towards our action). Therefore, we **drop this column** from both the datasets.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(columns='ROLE_TITLE')\ntest_df = test_df.drop(columns='ROLE_TITLE')\n\n# train_df.head() # to see if it has been dropped successfully","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.362514Z","iopub.execute_input":"2021-08-28T09:15:18.362804Z","iopub.status.idle":"2021-08-28T09:15:18.371956Z","shell.execute_reply.started":"2021-08-28T09:15:18.362777Z","shell.execute_reply":"2021-08-28T09:15:18.371063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['MGR_ID'].value_counts().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.373227Z","iopub.execute_input":"2021-08-28T09:15:18.373659Z","iopub.status.idle":"2021-08-28T09:15:18.389306Z","shell.execute_reply.started":"2021-08-28T09:15:18.373614Z","shell.execute_reply":"2021-08-28T09:15:18.388251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not dropping for the same reason as 'ROLL_ROLLUP_1'","metadata":{}},{"cell_type":"markdown","source":"* ### **Splitting the Training Dataset for Validation**\n    To try out different models and vary their parameters till we get the best tuning.\n   ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny=train_df['ACTION']\nX=train_df.drop('ACTION',axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.80,test_size=0.20, random_state=7)\n\n# X_train.head() \n# y_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.39081Z","iopub.execute_input":"2021-08-28T09:15:18.391229Z","iopub.status.idle":"2021-08-28T09:15:18.572519Z","shell.execute_reply.started":"2021-08-28T09:15:18.391196Z","shell.execute_reply":"2021-08-28T09:15:18.571665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### **Choosing a model for our problem**\n   Plan was to start with XGBoost, Random forests and then try others (for best fit) if time permits. But, I couldn't get time so I have    used **XGBoost**.  \n   \n   The [link](https://www.kaggle.com/alexisbcook/xgboost) of the article I used to learn and implement the model.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nXGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nXGB_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)],verbose=False)\n\nXGB_predictions = XGB_model.predict(X_valid) \n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:18.573773Z","iopub.execute_input":"2021-08-28T09:15:18.574272Z","iopub.status.idle":"2021-08-28T09:15:22.127172Z","shell.execute_reply.started":"2021-08-28T09:15:18.574241Z","shell.execute_reply":"2021-08-28T09:15:22.126357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note on the parameters**  \n\n `n_estimators` specifies how many times to go through the modeling cycle (which is the heart of the model)i.e. the number  of models that we include in the ensemble. Basically, the model would stop after this number has been reached (at max). Not setting this parameter properly may result in cases of ***overfitting*** and ***underfitting***.\n \n `early_stopping_rounds` offers a way to automatically find the ideal value for `n_estimators`. The value of `early_stopping_rounds` is set to 5 (So, it would stop after we find deterioting results consecutively for 5 times). This way, I was able to set the value of `n_estimators` high enough without worrying about overfitting.    \n \n `learning_rate` parameter helps to ensure that each sub-model added to the ensemble helps us less (thus avoiding overfitting due to the contributions of deep-models).\n \n \n\n","metadata":{}},{"cell_type":"code","source":"# To validate our model\nimport sklearn.metrics \n\nauc = sklearn.metrics.roc_auc_score(y_valid, XGB_predictions)\nprint(auc)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:22.128527Z","iopub.execute_input":"2021-08-28T09:15:22.129038Z","iopub.status.idle":"2021-08-28T09:15:22.142484Z","shell.execute_reply.started":"2021-08-28T09:15:22.129005Z","shell.execute_reply":"2021-08-28T09:15:22.141391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I varied the parameters to arrive at the ones I have put by checking AUC.","metadata":{}},{"cell_type":"markdown","source":"* ### **Cross Validation**\n  `cross_val_score()` function from `scikit-learn` can be used to generate the required folds for you.  \n  There is enough data (~32k rows) so that the split doesn't result in any non-randomness. So there is no harm in skipping this step.   \n  ","metadata":{}},{"cell_type":"markdown","source":"* ### **Applying the model on the test dataset**","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:22.143767Z","iopub.execute_input":"2021-08-28T09:15:22.14431Z","iopub.status.idle":"2021-08-28T09:15:22.157992Z","shell.execute_reply.started":"2021-08-28T09:15:22.144264Z","shell.execute_reply":"2021-08-28T09:15:22.156909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_final=test_df.drop('id',axis=1)\nXCB_final_preds = XGB_model.predict(X_test_final)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:22.160417Z","iopub.execute_input":"2021-08-28T09:15:22.160993Z","iopub.status.idle":"2021-08-28T09:15:22.345434Z","shell.execute_reply.started":"2021-08-28T09:15:22.160951Z","shell.execute_reply":"2021-08-28T09:15:22.344583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### **Submission**  ","metadata":{}},{"cell_type":"code","source":"XCB_output =  pd.DataFrame({'Id': 1+ X_test_final.index,\n                       'ACTION': XCB_final_preds})\nXCB_output.to_csv('XGB_submission.csv', index=False)\n\nXCB_output.head() ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:15:22.346766Z","iopub.execute_input":"2021-08-28T09:15:22.347311Z","iopub.status.idle":"2021-08-28T09:15:22.53948Z","shell.execute_reply.started":"2021-08-28T09:15:22.347276Z","shell.execute_reply":"2021-08-28T09:15:22.538522Z"},"trusted":true},"execution_count":null,"outputs":[]}]}